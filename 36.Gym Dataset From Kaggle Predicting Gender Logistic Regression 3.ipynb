{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea310906",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668cb20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 973\n",
      "Cleaned rows: 931\n",
      "Rows removed: 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, kstest, anderson\n",
    "\n",
    "\n",
    "#Grab data and put numerical vals in one table\n",
    "#################################################################################################################\n",
    "dataset_as_given = pd.read_csv(r\"d:\\Desktop\\Practice Python\\21.gym_members_exercise_tracking.csv\")\n",
    "#print(dataset_as_given)\n",
    "\n",
    "# Get column names\n",
    "column_names = dataset_as_given.columns\n",
    "#print(column_names)\n",
    "\n",
    "desired = [\"Age\", \"Weight (kg)\", \"Height (m)\", \"Max_BPM\", \"Avg_BPM\", \"Resting_BPM\", \n",
    "           \"Session_Duration (hours)\", \"Calories_Burned\", \"Fat_Percentage\", \"Water_Intake (liters)\", \"Workout_Frequency (days/week)\", \"Experience_Level\", \"BMI\"]\n",
    "\n",
    "dataset_as_given_numeric = dataset_as_given[desired]\n",
    "\n",
    "\n",
    "\n",
    "#Get Rid of Outliers\n",
    "#################################################################################################################\n",
    "\n",
    "# Copy the dataset so you keep the original intact\n",
    "cleaned_df = dataset_as_given.copy()\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = cleaned_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = cleaned_df[col].quantile(0.25)\n",
    "    Q3 = cleaned_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter out outliers\n",
    "    cleaned_df = cleaned_df[(cleaned_df[col] >= lower_bound) & (cleaned_df[col] <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Original rows:\", dataset_as_given.shape[0])\n",
    "print(\"Cleaned rows:\", cleaned_df.shape[0])\n",
    "print(\"Rows removed:\", dataset_as_given.shape[0] - cleaned_df.shape[0])\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "encoded_df = cleaned_df.copy()\n",
    "\n",
    "encoded_df['Gender'] = encoded_df['Gender'].map({\n",
    "    'Male': 1,\n",
    "    'Female': 0\n",
    "})\n",
    "\n",
    "workout_dummies = pd.get_dummies(encoded_df['Workout_Type'], prefix='Workout', drop_first=True)\n",
    "encoded_df = pd.concat([encoded_df.drop(columns=['Workout_Type']), workout_dummies], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcdfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   TRAIN/TEST SPLIT + SCALING\n",
    "#   This prepares your data for any ML model\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Define X (features) and y (target)\n",
    "# ------------------------------------------------------------\n",
    "# Gender must already be encoded as 0/1 in encoded_df\n",
    "X = encoded_df.drop(columns=['Gender'])\n",
    "y = encoded_df['Gender']\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Train/Test Split\n",
    "# ------------------------------------------------------------\n",
    "# test_size=0.2 means 20% of data is held out for testing\n",
    "# random_state=42 ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Scale the numeric features\n",
    "# ------------------------------------------------------------\n",
    "# Scaling helps models like Logistic Regression, SVM, KNN, Neural Networks\n",
    "# Tree-based models (Random Forest, XGBoost) do NOT require scaling,\n",
    "# but scaling does not harm them.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data ONLY, then transform both sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Optional: Print shapes to confirm everything looks right\n",
    "# ------------------------------------------------------------\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "print(\"\\nScaled versions created: X_train_scaled, X_test_scaled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e421f8",
   "metadata": {},
   "source": [
    "Logistic Regression is a supervised machine learning algorithm used for classification problems. <br>\n",
    "It predicts the the probability that an input belings to a specific class.<br>\n",
    "In our case the classes would be male and female. <br>\n",
    "Logistic Regression is used for binary classification (such as 0 or 1; True or False; Yes or No)<br><br>\n",
    "Logistic Regression has 3 main types:<br>\n",
    "1. Binomial Logistic Regression<br>\n",
    "This type is used when the dependant variable (in our case Gender) has only two possible categories.<br>\n",
    "It is the most common form of Logistic Regression and is used for binary classification problems.<br><br>\n",
    "\n",
    "2. Multinomial Logistic Regression<br>\n",
    "This type is used when the dependant variable (in our case Gender) has 3 or more possible categories that are not ordered.<br>\n",
    "As an example of this: Cat VS Dog VS Sheet -> they do not have a specific order.<br><br>\n",
    "\n",
    "3. Ordinal Logistic Regression<br>\n",
    "This type of Logistic Regression is used when the dependant variable (in our case that is Gender) has 3 or more categories it can be sorted into - and these categories have a natural order or ranking. As an example of this: Low VS Medium VS High. This specific model takes the order of the categories into account when modeling.<br><br>\n",
    "\n",
    "In order for the model to perfom well - or at least avoid certain issues that might stop it from performing well. There are some assumtions that must be met by the dataset before the model should be trained on it.<br><br>\n",
    "The main assumptions of Logistic Regression are:<br><br>\n",
    "\n",
    "1. Independent Observations -> this means that each of the columns we have should not be correlated to any of the other columns -> helps avoid multicolinearity.<br><br>\n",
    "2. Binary Dependant Variables -> this assumption means that the dataset tries to preddict the data assuming that the dependant variable (which is Gender in our case) can only take 2 forms (in our case these 2 forms would be Male and Female)<br><br>\n",
    "3. Linearity relationaship between independent variables and log odds -> the model assumes that a linear relationship exists between the independent variables and the log odds of the dependant variable -> meaning that the predictors affect the log odds of the predicted variable in a linear way.<br><br>\n",
    "4. No outliers -> the model assumes that the dataset contains no outliers - or at least no extreme outliers; Outliers in a Logistic Regression model can distort the estimation of the logistic regression coefficients.<br><br>\n",
    "5. Large sample size -> the models needs at least a certain amount of data to be able to be trained enough to give reasonable predictions -> otherwise it simply can not figure out the pattern of the data and be able to predict the dependant variable.<br><br>\n",
    "\n",
    "Ligistic Regression uses a Sigmoid Function<br><br>\n",
    "1. The sigmoid function is used to convert the raw output of the model into a probability value between 0 and 1.<br><br>\n",
    "2. The Sigmoid function takes any real number and maps it iinto the range 0 to 1 forming an 'S' shaped curve called the sigmoid curve. Because probabilities must lie between 0 and 1, the sigmoid function is perfect for this purpose.<br><br>\n",
    "3. In Logistic Regression, a threshold value (usually 0.5) is used to decide the class label.<br><br>\n",
    "- If the sigmoid output is same or above the threshold, the input is classified as Class 1.<br>\n",
    "- If it is below the threshold, the input is classified as Class 0.<br><br>\n",
    "The approach of the Sigmoid Function helps to transform continous input values into meaningful class predictions.<br><br>\n",
    "\n",
    "Terminologies used in Logistic Regression<br><br>\n",
    "1. Independent Variables -> These are the values we use to make predictions with<br>\n",
    "In our case we predict the Gender using vaiables such as Weight, Height, Water Intake, etc.<br><br>\n",
    "2. Dependant Variable -> The dependant variable is the variable we are predicting AKA the variable that is dependant on the independent variables in order to be predicted<br><br>\n",
    "3. Logistic Function -> This is the function that trasforms the Independent Variables (Weight, Height, etc.) into a probability between 0 and 1 which represents the likelihood that the Dependant Variable (Gender) is either 0 or 1.<br><br>\n",
    "4. Odds -> This is the ratio of the probability of an event happening to the probability of that event not happening. It differs from probability because probability is the ratio of occurances to total possibilities.<br><br>\n",
    "5. Log-Odds (Logit) -> The natural logarithm of the odds. In Logistic Regression, the log-odds are modeled as a linear combination of the independent variables and the intercept.<br><br>\n",
    "6. Coefficient -> These variables show how much the dependant variable is affected by the independent variables.\n",
    "7. Intercept -> The constant term in the Logistic Regression model which represents the log-odds when all independent variables are equal to 0.<br><br>\n",
    "8. Maximum Likelihood Estimation (MLE) -> This method is used to estimate the coefficients of the logistic regression model by maximizing the likelihood of observing the given data.<br><br>\n",
    "\n",
    "Implementation for Logistic Regression<br><br>\n",
    "\n",
    "https://www.geeksforgeeks.org/machine-learning/understanding-logistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOGISTIC REGRESSION\n",
    "# Good baseline model. Interpretable. Works well with scaling.\n",
    "# Look for balanced precision/recall and stable accuracy.\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   LOGISTIC REGRESSION — FULL MODEL EVALUATION SUITE\n",
    "#   This cell trains the model and evaluates it using:\n",
    "#     ✔ Accuracy\n",
    "#     ✔ Classification Report\n",
    "#     ✔ Confusion Matrix\n",
    "#     ✔ ROC Curve + AUC\n",
    "#     ✔ Precision–Recall Curve\n",
    "#     ✔ Coefficients (feature influence)\n",
    "#   Every section includes comments explaining what to look for.\n",
    "# ============================================================\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Train the Logistic Regression model\n",
    "# ------------------------------------------------------------\n",
    "# Logistic Regression works best with scaled data.\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "y_prob_lr = log_reg.predict_proba(X_test_scaled)[:, 1]  # needed for ROC/PR curves\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Accuracy Score\n",
    "# ------------------------------------------------------------\n",
    "# Accuracy = proportion of correct predictions.\n",
    "# Good for balanced datasets. If classes are imbalanced,\n",
    "# accuracy alone can be misleading.\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Classification Report\n",
    "# ------------------------------------------------------------\n",
    "# Shows precision, recall, and F1-score for each class.\n",
    "# Precision = how many predicted positives were correct.\n",
    "# Recall = how many actual positives were found.\n",
    "# F1 = balance between precision and recall.\n",
    "# Look for balanced values across both classes.\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Confusion Matrix\n",
    "# ------------------------------------------------------------\n",
    "# Shows EXACTLY where the model is making mistakes.\n",
    "# Diagonal = correct predictions.\n",
    "# Off-diagonal = errors.\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. ROC Curve + AUC Score\n",
    "# ------------------------------------------------------------\n",
    "# ROC curve shows how well the model separates the two classes.\n",
    "# AUC close to 1.0 = excellent.\n",
    "# AUC around 0.5 = random guessing.\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_lr)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='grey')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Precision–Recall Curve\n",
    "# ------------------------------------------------------------\n",
    "# Useful when one class is less common.\n",
    "# Look for curves that stay high (good precision)\n",
    "# and far right (good recall).\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob_lr)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision)\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Logistic Regression Coefficients\n",
    "# ------------------------------------------------------------\n",
    "# Shows how each feature influences the prediction.\n",
    "# Positive coefficient = pushes prediction toward class 1.\n",
    "# Negative coefficient = pushes prediction toward class 0.\n",
    "# Look for features with large absolute values.\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nLogistic Regression Coefficients (Feature Influence):\")\n",
    "print(coef_df)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=coef_df, x='Coefficient', y='Feature', palette='viridis')\n",
    "plt.title(\"Feature Influence (Logistic Regression Coefficients)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
